# CFS Scheduler

2023.01.16

---

## Introduction

이번 포스팅에서는 O(1) Scheduler의 단점을 보완하고 꽤 오랜 시간 사용된 CFS Scheduler에 대해 알아보자.

## Scheduling Class

그 전에 CFS가 도입된 리눅스 2.6.23 버전부터 도입된 ***Scheduling Class***를 잠시 짚고 넘어가자. Scheduling Class는 ***Scheduling Policy*를 *Class*로 만들어서 기존 커널 구조에 확장시키는 방향으로 커널에 추가적인 *Scheduling Policy*를 지원해주는 개념**이다. 스케줄링에 OO의 개념을 추가한 것이며 이를 통해 스케줄러를 조금 더 generic하게 만들었다.

## CFS Scheduler

O(1) Scheduler의 대안으로 등장한 것이 ***CFS(Completely Fair Scheduler)***이다. 이름에서 알 수 있듯이 이 스케줄러의 철학은 **모든 프로세스가 CPU를 최대한 공평하게 사용해야 한다**는 것이다.

### Weight

CFS는 각 프로세스에게 ***weight***라는 것을 부여한다. 그리고 **이 *weight*에 비례하여 *time slice*를 제공**한다. 

$$
C_{\tau_{i}}(t_1, t_2) = {W(\tau_i) \over S_\phi}*(t_2-t_1)
$$

위의 식에서 $S_\phi$는 전체 프로세스의 weight의 합, $W(\tau_i)$는 나의 weight를 의미하기 때문에 O(1) Scheduler와 달리 주변 프로세스의 특성을 반영하여 CPU time을 게산하는 것을 알 수 있다. 

![Untitled](CFS%20Scheduler%20078909f29c7d4173aadd982072650fe2/Untitled.png)

CFS가 추가적으로 weight라는 개념을 사용하지만 이전의 우선순위(nice value)를 사용하지 않는 것은 아니다. 대신 CFS는 이 **우선순위 값을 *weight*에 매핑**해준다. 우선순위 값이 하나 차이 날때마다 **사용하는 CPU 시간의 차이는 10%**로 정하였으며 이는 **각 단계마다 *weight*가 25% 정도 차이**나게 된다. 

이렇게 하면 앞에서 말했던 O(1)의 단점인 Different Switching Rate와 Unfair CPU Allocation은 해결이 된다.

### Selection Function

그러면 이제 CFS에서는 **어떻게 프로세스를 고를지**에 대하여 알아보자. CFS는 우선순위 대신 weight 개념을 도입하였고, 그 철학은 **모든 프로세스가 CPU를 공평하게 사용한다**는 것이다. 이를 위해 ***VirtualRunTime(VRT)***이라는 개념이 들어왔다. 핵심은 **내 프로세스가 얼마나 실행되었고, 얼마나 더 실행되어야 하는가**이다.

$$
VirtualRunTime(\tau, t)={weight_0 \over\ weight_\tau}\ *\ ExecutedRuntime(\tau, t)
$$

이 식을 살펴보면 $weigth_\tau$는 내 프로세스의 weight, $weight_0$는 nice value가 0의 weight를 의미한다. 그리고 ExecutedRuntime은 내 프로세스가 지금까지 쓴 CPU 시간을 의미한다. 우선순위가 높은 프로세스가 먼저 사용되어야 하니까 ***weight*가 큰 것**, 그리고 공평하게 CPU를 사용해야 하니까 **CPU를 덜 쓴 프로세스**를 골라야한다. 

즉, **CFS는 *VirtualRunTime*이 가장 작은 프로세스를 고른다.** 이렇게 선택된 프로세스에게 주어지는 time slice는 

$$
TimeSlice_{\tau_{i}}={weight_{\tau_i} \over \Sigma weight_{\tau_i}} * p
$$

### Data Structure

CFS는 Red-Black tree에 프로세스들을 저장하여 VRT가 가장 작은 프로세스를 $O(logn)$에 구한다(가장 왼쪽 밑의 노드). 각 노드의 키값은 각 프로세스의 VRT이다. 자료구조들을 도식화한 그림은 아래와 같다.

![Untitled](CFS%20Scheduler%20078909f29c7d4173aadd982072650fe2/Untitled%201.png)

runqueue는 커널 내의 `rq`라는 자료구조 안에 존재하며 **CPU당 하나씩 존재**한다. RT task들은 O(1)과 동일하게 FIFO, RR로 구현되어 있으며 `rt_rq`에 연결 리스트 형태로 구현되어 있다. 

우리의 프로세스들은 `cfs_rq`에 구현되어 있으며 `tasks_timeline`이라는 변수를 통해 RB-Tree의 root를 담고 있고 `rb_leftmost`를 통해 ***VirtualRunTime*이 가장 작은 프로세스에 대한 포인터를 저장**하고 있다. 각 프로세스 또한 CFS에 필요한 VirtualRunTime나 ExecutedRunTime을 계산하기 위한 여러 변수들을 저장하기 위해 PCB에 `sched_entity`라는 새로운 자료구조를 만들었다. `rb_leftmost`는 이 `sched_entity` 안의 `rb_node` 자료구조를 가리킨다. 

### Overview

위의 개념들을 이용하여 CFS에서 일어나는 일을 묘사하면 다음과 같다. 각 scheduling tick 마다

1. time slice를 하나 깎고 다 소진하면 바로 TIF_NEED_RESCHED를 세팅한다. 만약 다 소진하지 않았으면 **현재 *task*에 대한 *virtual runtime*을 업데이트**하여 현재 VRT보다 더 작은 VRT를 가진 task가 있다면 역시 flag를 세팅한다.
2. Interrupt handler 정리 후 flag가 세팅되어 있다면 ***schedule()*을 불러 *RB-Tree*의 맨 왼쪽 *task*를 가져온다.**

## Multiprocessor Scheduling

노는 CPU를 보고만 있을 수는 없기 때문에 ***Load Balancing*을 통하여 각 CPU의 *load*를 비슷하게 맞춰주는 작업이 *Multiprocessor Scheduling***이다.

![Untitled](CFS%20Scheduler%20078909f29c7d4173aadd982072650fe2/Untitled%202.png)

하지만 이게 현실적으로는 구현이 어려운데 RT에 프로세서 간 task를 옮긴다는 것은 모든 context와 자료구조를 옮겨야되어 overhead가 크고 기존에 사용하던 캐시가 날라가기 때문에 성능상에 저하가 있을 수 있기 때문이다.

리눅스에서는 **최대한 *idle*한 CPU가 없게 하고 *task*의 수의 차이를 최대한 작게 유지하는 방향**으로 Load Balancing을 구현하였으며 timer interrupt마다 실행할 수도 있고 스케줄링이 발생할 때마다 실행할 수도 있다.

## Preemptive Kernel & Nonpreemptive Kernel

마지막으로, **Kernel의 Preemption**에 대하여 알아보자.

- ***Preemptive Kernel**:* 프로세스가 Kernel mode에서 실행되고 있는 도중에 **다른 프로세스를 *preempt*(선점)하여 커널 코드가 모두 실행되지 않았음에도 *context*를 *switching*하는 커널** → 사용자에게로의 ***dispatch latency*를 줄이겠다**는 커널
- ***Nonpreemptive Kernel***: 스케줄러가 커널과 **협력하여서(*cooperatively*) *Kernel mode*가 모두 끝나고 Uesr mode로 오기까지 기다리는 커널** → **안정성을 중요시**하며 우리가 지금까지 다뤄온 커널.

현재 프로세스가 ***block*이 된다면 두 커널 모두 프로세스를 놓아야하지만**, interrupt handler가 우선순위가 더 높은 프로세스를 꺠우는 것처럼 **강제로 프로세스를 바꿔야한다면**, Nonpreemptive는 User mode로 올 때까지 기다리지만(Deferred), ***Preemptive*는 바로 바꿔버린다.** 그렇기 때문에 뒷정리 하는 것이 쉽지는 않다.

리눅스도 Preemptive Kernel을 지원은 한다. 하지만 커널 코드를 많이 바꾸고 싶지 않았기에 **특정 상황에만 *Preempt*가 가능하도록 구현**하였다. **커널이 판단했을 때 위험하면 *preempt*를 못하게 하며** 이를 나타내는 것이 ***lock***이다. lock이 되어 있으면 preempt를 할 수 없다. 일반적으로 ***interrupt handler*가 돌 적에는 *preempt*를 못하고**, ***system call, exception handler*가 도는 상황에서만 *preempt*를 가능하게 구현**하였다.